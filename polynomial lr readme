Q1)what is polynomial regression?
Simple idea:
Polynomial Linear Regression lets us model non-linear data using a linear model by adding powers of the input feature.
It’s still linear in the parameters, but non-linear in the input.


Q2)what is the formula for polynomial lr?
y = b0 + b1 * x + b2 * x^2 
y  = Predicted output (target value)
x  = Input feature (independent variable)
b0 = Intercept (bias term)
b1 = Coefficient for x (linear term)
b2 = Coefficient for x^2 (quadratic term)

Q3)Why Is It Called "Polynomial Linear Regression" Even Though It’s Curved?
Because it is linear in the coefficients — not in the features.
Look at this Polynomial Regression Equation:
y = b0 + b1 * x + b2 * x^2 + b3 * x^3
It has:
Non-linear features: x^2, x^3, etc.
But the equation is linear in terms of the parameters: b0, b1, b2, b3
That means:
We're still solving using linear algebra methods like in simple/multiple linear regression.
There's no non-linear optimization involved — just regular least squares.

Q4) what r the main fns??
i)fit()
poly.fit(x)
This does nothing visible right now — it just learns what it needs to do.
In this case, it figures out:
i need to add x, x^2, and a bias term (1).

ii)transform()
x_poly = poly.transform(x)
print(x_poly)
Now it actually adds new columns:
[[1. 2. 4.]
 [1. 3. 9.]
 [1. 4. 16.]]
What happened?
1. → bias term (always added)
2., 3., 4. → x
4., 9., 16. → x²
So each row becomes: [1, x, x²]

iii)fit_transform()
x_poly = poly.fit_transform(x)
You get the same result:
[[1. 2. 4.]
 [1. 3. 9.]
 [1. 4. 16.]]



